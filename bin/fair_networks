#!/usr/bin/env python

import sys
import numpy as np
import tensorflow as tf
import time
import pandas
import sklearn.svm as svm
from termcolor import colored
import logging

from fair.fn.model import Model
from fair.utils.options import Options
from fair.fn.fair_networks_training import FairNetworksTraining
from fair.datasets.yale_b_dataset import YaleBDataset



def print_stats(session, model, dataset):
    print("Learning rate:")
    print(session.run(model.optimizer._learning_rate_tensor))

    train_xs, train_ys, train_s = dataset.train_all_data()
    test_xs, test_ys, test_s = dataset.test_all_data()

    train_feed = {model.x: train_xs, model.y: train_ys, model.s: train_s}
    test_feed = {model.x: test_xs, model.y: test_ys, model.s: test_s}


    print("loss and accuracy:")
    model.print_loss_and_accuracy(session, train_feed_dict = train_feed, test_feed_dict = test_feed)

    print(colored("\nConfusion matrix -- Train:", attrs=['bold']))
    model.print_confusion_matrix(session, feed_dict = train_feed)

    print(colored("\nConfusion matrix -- Test:", attrs=['bold']))
    model.print_confusion_matrix(session, feed_dict = test_feed)

def print_processed_data(session, model, dataset):
    train_xs, train_ys, train_s = dataset.train_all_data()
    result, header = process_data(session, model, train_xs, train_ys, train_s)

    to_path = opts.eval_data_path+'_train.csv'
    print(colored("Saving data representations onto {}".format(to_path), "green"))
    pandas.DataFrame(result, columns=header).to_csv(to_path, index=False)

    test_xs, test_ys, test_s = dataset.test_all_data()
    result_test, header_test = process_data(session, model, test_xs, test_ys, test_s)

    to_path = opts.eval_data_path+'_test.csv'
    print(colored("Saving data representations onto {}".format(to_path), "green"))
    pandas.DataFrame(result_test, columns=header_test).to_csv(to_path, index=False)


def process_data(session, model, xs, ys, s):
    noise = np.random.uniform(size=xs.shape)
    model_data_representation = session.run(model.model_last_hidden_layer, feed_dict={
        model.x:xs, model.y:ys, model.s:s, model.noise:noise
        })

    result = np.hstack((model_data_representation, s, ys))
    h_header = ["h_"+str(index) for index in range(len(model_data_representation[0]))]
    s_header = ["s_"+str(index) for index in range(len(s[0]))]
    y_header = ["y_"+str(index) for index in range(len(ys[0]))]
    header = h_header + s_header + y_header
    return result, header


def init_model(opts, session, saver, writer):
    if opts.resume_learning:
        model_to_resume = opts.input_fname()

        if opts.log_level <= logging.INFO:
            logging.info(colored("Restoring model: %s" % (model_to_resume), 'yellow'))

        saver.restore(session, model_to_resume)

        graph_fairness_importance = session.run(
            tf.get_default_graph().get_tensor_by_name("fairness_importance:0"))

        if graph_fairness_importance != opts.fairness_importance:
            print(colored("Warning:", "yellow") +
                  "Fairness importance changed by the options, but it is part of the model.")
            print("graph: {} opts: {}".format(graph_fairness_importance, opts.fairness_importance))
            # exit(1)
    else:
        print(colored("Initializing a new model", 'yellow'))
        init = tf.global_variables_initializer()
        session.run(init)
        writer.add_graph(session.graph)

def print_data_sample(options, session, model):
    xs, ys, s = options.dataset.train_all_data()
    print(colored("xs", "yellow"))
    print(xs[:15, :])
    print(colored("ys", "yellow"))
    print(ys[:15, :])
    print(colored("s", "yellow"))
    print(s[:15, :])

def print_out_sample(options, session, model):
    xs, ys, s = options.dataset.train_all_data()
    print(colored("y", "yellow"))
    print( "{}".format(session.run(model.y_out, feed_dict={model.x:xs[:15,:]}) ))
    print(colored("s", "yellow"))
    print( "{}".format(session.run(model.s_out, feed_dict={model.x:xs[:15,:]}) ))

def print_model_information(options, session, model, requested_info):
    assert requested_info in ['epoch', 'variables', 'data-sample', 'out-sample'], "--get-info specifies an supported value (should not get here)"

    if requested_info == 'epoch':
        print(int(session.run( model.epoch[0])))
    elif requested_info == 'variables':
        model.print_weights(session)
    elif requested_info == 'out-sample':
        print_out_sample(options, session, model)
    else:
        print_data_sample(options, session, model)


# --------------------------------------------------------------------------------
# main
# --------------------------------------------------------------------------------


opts = Options(sys.argv)
tf.set_random_seed(opts.random_seed)
np.random.seed(opts.random_seed)

if opts.log_level <= logging.INFO:
    print(colored("Initialized system based on config:", "green"))
    opts.print_config()

    print(colored("Loaded dataset %s" % opts.dataset.name(), "green"))
    opts.dataset.print_stats()

print(colored("Loaded dataset %s" % opts.dataset.name(), "green"))

optimizer = tf.train.AdamOptimizer(opts.learning_rate)
model = Model(opts, optimizer)

# session = tf.Session()
# saver = tf.train.Saver()
# writer = tf.summary.FileWriter(logdir=opts.log_fname())

# print("Initializing model")
# init_model(opts, session, saver, writer)
# print("Model checkpoint: {}".format(opts.resume_ckpt))
# print("Model initialized")


# if opts.eval_stats:
#     print_stats(session, model, opts.dataset)
# elif opts.eval_data_path != None:
#     print("Evaluating representations")
#     print_processed_data(session, model, opts.dataset)
# elif opts.get_info != None:
#     print_model_information(opts, session, model, opts.get_info)
# else:
#     fnt = FairNetworksTraining(opts, session, model, saver, writer)
#     fnt.training_loop()


def log_losses(epoch, session, model, x, y):
    accuracy = session.run(model.y_accuracy, feed_dict={
                           model.x: x, model.y: y})
    loss = session.run(model.y_loss, feed_dict={model.x: x, model.y: y})
    print("epoch: {} loss: {:7.4f} accuracy: {:7.4f}".format(epoch, loss, accuracy))


dataset = YaleBDataset('./data')
dataset.load_all()
x, y, s = dataset._traindata
xt, yt, st = dataset._testdata


session = tf.Session()
init = tf.global_variables_initializer()
init_local = tf.local_variables_initializer()

session.run(init)
session.run(init_local)
batch_size = 16

writer = tf.summary.FileWriter('logdir')
writer.add_graph(session.graph)

for epoch in range(200):
    batch_current = 0
    while batch_current < len(x):
        x_batch = x[batch_current:(batch_current+batch_size)]
        y_batch = y[batch_current:(batch_current+batch_size)]
        batch_current += batch_size

        session.run(model.y_train_step, feed_dict={
                    model.x: x_batch, model.y: y_batch})

    if epoch % 10 == 0:
        print("Train")
        log_losses(epoch, session, model, x, y)

        print("Test")
        log_losses(epoch, session, model, xt, yt)


