#!/usr/bin/env python

import sys
sys.path.append(r"C:\Users\Visitor\Documents\fair-networks\packages")

from fair.datasets.adult_dataset import AdultDataset
from fair.datasets.synth_easy2_dataset import SynthEasy2Dataset

import numpy as np
import tensorflow.compat.v1 as tf
import time
import pandas
import sklearn.svm as svm
import argparse
import math


def create_layer(x, num_out, activation, prefix='h', num=0, 
                 initializer=tf.truncated_normal_initializer,
                 random_units=0):
    variables = []
    num_in = x.shape[1]
    w = tf.get_variable("{}-{}-w".format(prefix, num), dtype=tf.float32, shape=[num_in+random_units, num_out], initializer=initializer())
    b = tf.get_variable("{}-{}-b".format(prefix, num), dtype=tf.float32, shape=[num_out], initializer=initializer())
    if random_units != 0:
        x_rand = tf.random_normal(name="{}-{}-b".format(prefix, num), shape=[tf.shape(x)[0], random_units])
        x = tf.concat([x, x_rand], axis=1)
    variables.extend([w, b])
    out = activation(tf.matmul(x, w) + b)
    return out, variables


if __name__ == '__main__':
    ## argument handling
    parser = argparse.ArgumentParser()
    parser.add_argument('dataset', choices=['adult', 'synth-easy2'], help="dataset to be loaded")
    parser.add_argument('-l', '--learning-rate', type=float, default=1e-4, help="Specifies the (initial) learning rate")
    parser.add_argument('-f', '--fairness-importance', type=float, default=5.0, help="Specify how important is fairness w.r.t. the error")
    parser.add_argument('-b', '--batch-size', type=int, default=100, help="Specifies the batch size to be used")
    parser.add_argument('-e', '--num-epochs', type=int, default=2000, help="Specifies the number of epochs the model will be trained for")
    result = parser.parse_args(sys.argv[1:])

    ## dataset loading
    datasets = {'adult': AdultDataset, 'synth-easy2': SynthEasy2Dataset}
    dataset_base_path = 'data/'
    dataset = datasets[result.dataset](dataset_base_path)
    x_train, y_train, s_train = dataset.train_all_data()
    x_test, y_test, s_test = dataset.test_all_data()

    num_features = dataset.num_features()
    num_s_labels = dataset.num_s_columns()
    num_y_labels = dataset.num_y_columns()

    ## hyperparameters
    learning_rate = result.learning_rate
    h_sizes = [5, 5]
    s_sizes = [5, 5]
    y_sizes = [5, 5]
    r_units = [5, 5]
    batch_size = result.batch_size
    fairness_importance = result.fairness_importance
    num_epochs = result.num_epochs

    ## model def
    x_ph = tf.placeholder(tf.float32, shape=[None, num_features], name="x")
    y_ph = tf.placeholder(tf.float32, shape=[None, num_y_labels], name="y")
    s_ph = tf.placeholder(tf.float32, shape=[None, num_s_labels], name="s")

    h_vars = []
    input_operation = x_ph
    for i, h_size in enumerate(h_sizes):
        out, variables = create_layer(input_operation, h_size, tf.nn.sigmoid, prefix="h", num=i, random_units=r_units[i])
        input_operation = out
        h_vars.extend(variables)
    last_h = input_operation

    s_vars = []
    for i, s_size in enumerate(s_sizes):
        out, variables = create_layer(input_operation, s_size, tf.nn.sigmoid, prefix="s", num=i)
        input_operation = out
        s_vars.extend(variables)
    s_out, variables = create_layer(input_operation, num_s_labels, tf.identity, prefix='s', num=i+1)
    s_vars.extend(variables)

    y_vars = []
    input_operation = last_h
    for i, y_size in enumerate(y_sizes):
        out, variables = create_layer(input_operation, y_size, tf.nn.sigmoid, prefix="y", num=i)
        input_operation = out
        y_vars.extend(variables)
    y_out, variables = create_layer(input_operation, num_y_labels, tf.identity, prefix="y", num=i+1)
    y_vars.extend(variables)

    y_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ph, logits=y_out))
    s_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=s_ph, logits=s_out))
    h_loss = y_loss - fairness_importance * s_loss

    optimizer = tf.train.AdagradOptimizer(learning_rate)
    h_step = optimizer.minimize(h_loss, var_list=h_vars)
    s_step = optimizer.minimize(s_loss, var_list=s_vars)
    y_step = optimizer.minimize(y_loss, var_list=y_vars)

    ## tf housekeeping
    init = tf.initializers.global_variables()
    sess = tf.Session()
    sess.run(init)

    ## training loop
    num_batches_per_epoch = math.ceil(len(x_train)/batch_size)
    for i in range(num_epochs):
        for j in range(num_batches_per_epoch):
            start = batch_size * j 
            end = batch_size * (j+1)

            sess.run([y_step], feed_dict={x_ph: x_train[start:end], y_ph: y_train[start:end]})
            sess.run([s_step], feed_dict={x_ph: x_train[start:end], s_ph: s_train[start:end]})
            sess.run([h_step], feed_dict={x_ph: x_train[start:end], y_ph: y_train[start:end], s_ph: s_train[start:end]})

        if i % 10 == 0:
            y_loss_value = sess.run(y_loss, feed_dict={x_ph: x_test, y_ph: y_test})
            s_loss_value = sess.run(s_loss, feed_dict={x_ph: x_test, s_ph: s_test})
            h_loss_value = sess.run(h_loss, feed_dict={x_ph: x_test, y_ph: y_test, s_ph: s_test})
            print('Epoch {:4} y loss: {:07.6f} s loss: {:07.6f} h loss: {:07.6f}'.format(i, y_loss_value, s_loss_value, h_loss_value))

    repr_h_train = sess.run(last_h, feed_dict={x_ph: x_train})
    repr_h_test = sess.run(last_h, feed_dict={x_ph: x_test})


    result = np.hstack((repr_h_train, s_train, y_train))
    h_header = ["h_"+str(index)
                for index in range(len(repr_h_train[0]))]
    s_header = ["s_"+str(index) for index in range(len(s_train[0]))]
    y_header = ["y_"+str(index) for index in range(len(y_train[0]))]
    header = h_header + s_header + y_header

    pandas.DataFrame(result, columns=header).to_csv("simple_repr_train.csv", index=False)

    result = np.hstack((repr_h_test, s_test, y_test))
    h_header = ["h_"+str(index)
                for index in range(len(repr_h_test[0]))]
    s_header = ["s_"+str(index) for index in range(len(s_test[0]))]
    y_header = ["y_"+str(index) for index in range(len(y_test[0]))]
    header = h_header + s_header + y_header

    pandas.DataFrame(result, columns=header).to_csv(
        "simple_repr_test.csv", index=False)
