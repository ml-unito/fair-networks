#!/usr/bin/env python

import glob
import json
import re
import os
import sys
import itertools
from termcolor import colored
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.preprocessing import MinMaxScaler
import numpy as np


def results_summary(results, classifier_id='forest', mine=False):
    """ Returns a dictionary containing the results on the validation set of the
        logistic regression classifier. 
        
    The dictionary has the following keys:
    fny: fair networks performances (accuracy of lr in predicting y on the 
        representations learnt by fair networks)
    ppy: (++y) best possible performances on y (results on the original representations)
    mmy: (--y) worst possible performances on y (results on random representations)

    fns: fair networks performances on s
    pps: (++s) best possible performances (results on the random representations)
    mms: (--s) worst possible performances (results on the original representations)
    """
    fny = results['performances']['fair_networks_repr'][classifier_id]['y']['val']
    ppy = results['performances']['original_repr'][classifier_id]['y']['val']
    mmy = results['performances']['random_networks_repr'][classifier_id]['y']['val']

    fns = results['performances']['fair_networks_repr'][classifier_id]['s']['val']
    pps = results['performances']['random_networks_repr'][classifier_id]['s']['val']
    mms = results['performances']['original_repr'][classifier_id]['s']['val']

    if mine:
        mi  = results['performances']['fair_networks_repr']['mine']['s']['train']
        return { 'fny': fny, 'ppy': ppy, 'mmy': mmy, 'fns': fns, 'pps': pps, 'mms': mms, 'mi': mi}

    else:
        return {'fny': fny, 'ppy': ppy, 'mmy': mmy, 'fns': fns, 'pps': pps, 'mms': mms}

def results_score(summary):
    """ Returns an evaluation of the performances summarized by the summary parameter.
        The evaluation is given as a scalar computed as the sum of performances on y 
        and on s. 
    
     Performances are computed as percentage of possible gain. Possible gain is evaluated
     as the best possible performance (e.g., ppy) minus the worst possible performance
     (e.g., mmy). The gain is computed as the actual performance (e.g., fny) minus the
     worst possible performance.
    
     Performances on s are computed in the opposite direction since the best possible
     performance are a lower number w.r.t. the best possible performances.
     """
    perf_y =  (summary['fny']-summary['mmy'])/(summary['ppy'] - summary['mmy'])
    perf_s = (summary['mms']-summary['fns'])/(summary['mms'] - summary['pps'])

    return perf_y + perf_s

def match_performance(s):
    """
    Matches the given string to see if it is the name of a file containing
    a performance evaluation at a given epoch.

    Returns a tuple (match, epoch): 
        - match is a boolean specifying if the string matches the expected format. 
        - epoch is the epoch to which the performances refer to (or None if the 
          string does not match).
    """
    pattern = re.compile(r'.*performances(\d+)\.json')
    match = pattern.match(s)

    if match:
        return (True, int(match.groups()[0]))

    if match == None:
        match = re.match(r'.*performancesfinal\.json', s)
        return (True, 3000)

    return (False, None)

def parse_epoch(s):
    """
    Given the name of a file containing a performance evaluation at a 
    given epoch, returns the epoch to which the name refers to.
    """
    return match_performance(s)[1]

def pareto_set(dirname, regression_study=False, mine=False):
    """
    Given a directory name, it analyses the set of performance files in that
    directory (there should be one file for each one of the epochs under study).

    It returns the pareto set of the performances evaluated as the sum of the
    two objectives: 1) % of gain on the y variable w.r.t. the base line (majority 
    class) and 2) % of gain in unpredictability of s w.r.t. the base line (the
    original representation of the data).

    The two objectives are equally weighted, thus the pareto set is the set of
    experiments (epochs) for which obj1 + obj2 is maximal.
    """
    files = glob.glob(os.path.join(dirname, 'performances*.json'))
    filter( lambda x: match_performance(x)[0], files )

    files.sort(key=parse_epoch)

    performances = []
    for fname in files:
        epoch = parse_epoch(fname)

        if epoch == None:
            continue

        with open(fname, 'r') as f:
            results = json.load(f)
        try:
            temp = results['performances']
        except KeyError:
            print(colored('{} has no performances. Skipping.'.format(fname), 'red'))
            continue
        summary = results_summary(results, mine=mine)
        performances.append({
            'score': results_score(summary), 
            'summary': summary, 
            'file': fname, 
            'epoch': epoch
            })
    if len(performances) == 0:
        print("No performances found for dir: {}".format(dirname))
        return None

    def get_score(el): return el['score']

    performances.sort(key=get_score, reverse=True)
    perf_grouped = itertools.groupby(performances, key=get_score)
    return next(perf_grouped)

def regression_study(results, scaling=False):
    """
    Given a directory name, get results for the s classifier and the mutual information estimator;
    then plot them one against each other in a regression study.
    """
    noise_s_perfs = []
    noise_mi_perfs = []
    nonoise_s_perfs = []
    nonoise_mi_perfs = []
    for result in results:
        _, elem = result
        fst, *_ = list(elem)
        exp_name = fst['file']
        # order of if statements matters. 'nonoise' will be matched by looking for 'noise'
        if 'nonoise' in exp_name:
            nonoise_mi_perf = fst['summary']['mi']
            if not np.isnan(nonoise_mi_perf):
                nonoise_s_perfs.append(fst['summary']['fns'])
                nonoise_mi_perfs.append(fst['summary']['mi'])
        elif 'noise' in exp_name:
            noise_mi_perf = fst['summary']['mi']
            if not np.isnan(noise_mi_perf):
                noise_s_perfs.append(fst['summary']['fns'])
                noise_mi_perfs.append(fst['summary']['mi'])
        else:
            print(colored('Warning: file {} did not match noise, nonoise'.format(result['file']), 'yellow'))

    #model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)
    #model = LinearRegression()
    model = SVR(kernel='linear', C=10, gamma='auto', degree=2, epsilon=.1,
               coef0=1)
    # prepare and concat data
    nonoise_mi_perfs = np.array(nonoise_mi_perfs)
    nonoise_s_perfs  = np.array(nonoise_s_perfs)
    noise_mi_perfs   = np.array(noise_mi_perfs)
    noise_s_perfs    = np.array(noise_s_perfs)
    all_mi_data = np.hstack([nonoise_mi_perfs, noise_mi_perfs])
    all_s_data = np.hstack([nonoise_s_perfs, noise_s_perfs])
    # sort and reshape data
    sorted_idx = np.argsort(all_mi_data)
    all_mi_data = all_mi_data[sorted_idx].reshape(-1, 1)
    all_s_data = all_s_data[sorted_idx].reshape(-1, 1)
    # scale
    if scaling:
        mi_scaler = MinMaxScaler()
        all_mi_data = mi_scaler.fit_transform(all_mi_data)
        s_scaler = MinMaxScaler()
        all_s_data = s_scaler.fit_transform(all_s_data)
    # train and plot
    model.fit(all_mi_data, all_s_data)
    pred_s = model.predict(all_mi_data).reshape(-1, 1)
    if scaling: # revert scaling to plot, if necessary
        s_line_data = s_scaler.inverse_transform(pred_s)
        mi_line_data = mi_scaler.inverse_transform(all_mi_data)
        s_nonoise_scatter_data = s_scaler.inverse_transform(nonoise_s_perfs.reshape(-1, 1))
        s_noise_scatter_data = s_scaler.inverse_transform(noise_s_perfs.reshape(-1, 1))
        mi_nonoise_scatter_data = mi_scaler.inverse_transform(nonoise_mi_perfs.reshape(-1, 1))
        mi_noise_scatter_data = mi_scaler.inverse_transform(noise_mi_perfs.reshape(-1, 1))
    else:
        s_line_data = pred_s
        mi_line_data = all_mi_data
        s_nonoise_scatter_data = nonoise_s_perfs.reshape(-1, 1)        
        s_noise_scatter_data = noise_s_perfs.reshape(-1, 1)
        mi_nonoise_scatter_data = nonoise_mi_perfs.reshape(-1, 1)
        mi_noise_scatter_data = noise_mi_perfs.reshape(-1, 1)
    # plot regression line
    plt.plot(mi_line_data, s_line_data, '--', color='black', linewidth='2')
    # plot nonoise performance
    nonoise_scatter = plt.scatter(mi_nonoise_scatter_data, 
                      s_nonoise_scatter_data,
                      c='orange', marker='*', alpha=0.5,
                      label='Adversarial Strategy')
    # plot noise performance
    noise_scatter = plt.scatter(mi_noise_scatter_data,
                    s_noise_scatter_data, marker='x',
                    c='cyan', alpha=0.5,
                    label='Adversarial Strategy with Noise Module')
    plt.ylim(0.35, 1.05)
    plt.xlabel('I(X\'; S) (MINE-estimated, nats)')
    plt.ylabel('Classifier accuracy on sensible variable')
    plt.legend(['Regression Line', nonoise_scatter._label, noise_scatter._label])
    plt.savefig('regression.png')
    

def print_summary(result):
    """
    Prints a summary of the given result
    """
    _, elem = result
    fst, *rest = list(elem)

    summary = "{score} file: {fname} size: {size}".format(
        score = colored(fst['score'], 'yellow'),
        fname = colored(fst['file'], 'green'),
        size = len(rest) + 1)

    perfs = "\tfny: {} ++y: {:6.4} --y: {:6.4} fns: {} ++s: {:6.4} --s: {:6.4} mi: {:6.6}".format(
            colored("{:6.4}".format(fst['summary']['fny']), 'white') ,
            fst['summary']['ppy'],
            fst['summary']['mmy'],
            colored("{:6.4}".format(fst['summary']['fns']), 'white'),
            fst['summary']['pps'],
            fst['summary']['mms'], fst['summary']['mi'])

    print(summary)
    print(colored(perfs, 'grey'))
    print()


# Main

results = []
do_regression = bool(sys.argv[1])
mine = bool(sys.argv[2])
print(colored('do_regression: {}'.format(do_regression), 'green'))
for dirname in sys.argv[3:]:
    result = pareto_set(dirname)
    if result == None:
        continue
    results.append(pareto_set(dirname))

def get_score(result): return result[0]

results.sort(key=get_score, reverse=True)

if do_regression:
    regression_study(results)
else:
    for result in results:
        print_summary(result)
    



