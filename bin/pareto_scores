#!/usr/bin/env python

import glob
import json
import re
import os
import sys
import itertools
from termcolor import colored



def results_summary(results, classifier='lr'):
    """ Returns a dictionary containing the results on the validation set of the
        logistic regression classifier. 
        
    The dictionary has the following keys:
    fny: fair networks performances (accuracy of lr in predicting y on the 
        representations learnt by fair networks)
    ppy: (++y) best possible performances on y (results on the original representations)
    mmy: (--y) worst possible performances on y (results on random representations)

    fns: fair networks performances on s
    pps: (++s) best possible performances (results on the random representations)
    mms: (--s) worst possible performances (results on the original representations)
    """
    fny = results['performances']['fair_networks_repr'][classifier]['y']['val']
    ppy = results['performances']['original_repr'][classifier]['y']['val']
    mmy = results['performances']['random_networks_repr'][classifier]['y']['val']

    fns = results['performances']['fair_networks_repr'][classifier]['s']['val']
    pps = results['performances']['random_networks_repr'][classifier]['s']['val']
    mms = results['performances']['original_repr'][classifier]['s']['val']

    return { 'fny': fny, 'ppy': ppy, 'mmy': mmy, 'fns': fns, 'pps': pps, 'mms': mms }

def results_score(summary):
    """ Returns an evaluation of the performances summarized by the summary parameter.
        The evaluation is given as a scalar computed as the sum of performances on y 
        and on s. 
    
     Performances are computed as percentage of possible gain. Possible gain is evaluated
     as the best possible performance (e.g., ppy) minus the worst possible performance
     (e.g., mmy). The gain is computed as the actual performance (e.g., fny) minus the
     worst possible performance.
    
     Performances on s are computed in the opposite direction since the best possible
     performance are a lower number w.r.t. the best possible performances.
     """
    try:
        perf_y =  (summary['fny']-summary['mmy'])/(summary['ppy'] - summary['mmy'])
    except ZeroDivisionError:
        perf_y = 1
    perf_s = (summary['mms']-summary['fns'])/(summary['mms'] - summary['pps'])

    return perf_y + perf_s

def match_performance(s):
    """
    Matches the given string to see if it is the name of a file containing
    a performance evaluation at a given epoch.

    Returns a tuple (match, epoch): 
        - match is a boolean specifying if the string matches the expected format. 
        - epoch is the epoch to which the performances refer to (or None if the 
          string does not match).
    """
    pattern = re.compile(r'.*performances(\d+)\.json')
    match = pattern.match(s)

    if match:
        return (True, int(match.groups()[0]))

    if match == None:
        match = re.match(r'.*performancesfinal\.json', s)
        return (True, 3000)

    return (False, None)

def parse_epoch(s):
    """
    Given the name of a file containing a performance evaluation at a 
    given epoch, returns the epoch to which the name refers to.
    """
    return match_performance(s)[1]

def average_summaries(summary_list):
    """
    Given a list of summaries, average their results. This is useful
    if you want to get the best models according to multiple classifiers.
    
    Note that this method iterates on the keys found in the very first summary
    dict, and there is no explicit check for dictionary key correspondance.
    """
    key_list = summary_list[0].keys()
    avg_summary = {}

    for key in key_list:
        tmp_value = []
        for summary in summary_list:
            tmp_value.append(summary[key])
        if key == 'fns':
            avg_summary[key] = max(tmp_value)
        else:
            avg_summary[key] = sum(tmp_value) / len(summary_list)

    return avg_summary

def pareto_set(dirname):
    """
    Given a directory name, it analyses the set of performance files in that
    directory (there should be one file for each one of the epochs under study).

    It returns the pareto set of the performances evaluated as the sum of the
    two objectives: 1) % of gain on the y variable w.r.t. the base line (majority 
    class) and 2) % of gain in unpredictability of s w.r.t. the base line (the
    original representation of the data).

    The two objectives are equally weighted, thus the pareto set is the set of
    experiments (epochs) for which obj1 + obj2 is maximal.
    """
    files = glob.glob(os.path.join(dirname, 'performances*.json'))
    filter( lambda x: match_performance(x)[0], files )

    files.sort(key=parse_epoch)

    performances = []
    for fname in files:
        epoch = parse_epoch(fname)

        if epoch == None:
            continue

        with open(fname, 'r') as f:
            results = json.load(f)
        try:
            temp = results['performances']
        except KeyError:
            print(colored('{} has no performances. Skipping.'.format(fname), 'red'))
            continue
        summary_lr = results_summary(results, classifier='lr')
        summary_forest = results_summary(results, classifier='forest')
        avg_score = (results_score(summary_lr) + results_score(summary_forest)) / 2
        avg_summary = average_summaries([summary_forest, summary_lr])
        performances.append({
            'score': avg_score, 
            'summary': avg_summary, 
            'file': fname, 
            'epoch': epoch
            })

    if len(performances) == 0:
        print("No performances found for dir: {}".format(dirname))
        return None

    def get_score(el): return el['score']

    performances.sort(key=get_score, reverse=True)
    perf_grouped = itertools.groupby(performances, key=get_score)
    return next(perf_grouped)


def print_summary(result):
    """
    Prints a summary of the given result
    """
    _, elem = result
    fst, *rest = list(elem)

    summary = "{score} file: {fname} size: {size}".format(
        score = colored(fst['score'], 'yellow'),
        fname = colored(fst['file'], 'green'),
        size = len(rest) + 1)

    perfs = "\tfny: {} ++y: {:6.4} --y: {:6.4} fns: {} ++s: {:6.4} --s: {:6.4}".format(
            colored("{:6.4}".format(fst['summary']['fny']), 'white') ,
            fst['summary']['ppy' ],
            fst['summary']['mmy'],
            colored("{:6.4}".format(fst['summary']['fns']), 'white'),
            fst['summary']['pps'],
            fst['summary']['mms'])

    print(summary)
    print(colored(perfs, 'grey'))
    print()


# Main

results = []
for dirname in sys.argv[1:]:
    print(dirname)
    result = pareto_set(dirname)
    if result == None:
        continue
    results.append(pareto_set(dirname))

def get_score(result): return result[0]

results.sort(key=get_score, reverse=True)

for result in results:
    print_summary(result)
    



