#!/usr/bin/env python

import pandas
import sklearn.model_selection as ms
import sklearn.svm as svm
import sklearn.tree as tree
import sklearn.linear_model as lm
import sklearn.ensemble as ens
import numpy as np
import os
import sys
import json
import traceback
import re
import logging
from termcolor import colored

RANDOM_SEED=42

from fair.fn.model import Model
from fair.utils.options import Options
from sklearn.metrics import confusion_matrix
from vfae_louizos.example import discrimination_noprob
# from mi_estimation.MINE import MINE

# Reads representations from experiments in the given directory (must be the root of the 
# experiments directory, the one containg the config.json file) and writes a "perfomrances.json"
# file containing the performances of several learning algorithms on the representations built
# for the experiment.

def read_commit_id(file_path):
    if os.path.exists(file_path):
        return open(file_path, "r").read()
    else:
        return "N/A"

def accuracy(pred, y):
    return np.average(y == pred)

def eval_accuracies_on_representation(train_path, val_path, test_path):
    df_train = pandas.read_csv(train_path)
    y_train = df_train.filter(regex="y.*").values
    s_train = df_train.filter(regex="s.*").values
    h_train = df_train.filter(regex="h.*").values

    df_val = pandas.read_csv(val_path)
    y_val = df_val.filter(regex="y.*").values
    s_val = df_val.filter(regex="s.*").values
    h_val = df_val.filter(regex="h.*").values

    df_test = pandas.read_csv(test_path)
    y_test = df_test.filter(regex="y.*").values
    s_test = df_test.filter(regex="s.*").values
    h_test = df_test.filter(regex="h.*").values

    # h_train, h_test, s_train, s_test, y_train, y_test = ms.train_test_split(h_columns, s_columns, y_columns, random_state=RANDOM_SEED)
    
    # mine_performances = estimate_mi_mine(h_train, s_train, h_val, s_val, h_test, s_test) # needs one-hot encodings, so it should be called here

    y_train = np.argmax(y_train, axis=1)
    y_val = np.argmax(y_val, axis=1)
    y_test = np.argmax(y_test, axis=1)
    s_train = np.argmax(s_train, axis=1)
    s_val = np.argmax(s_val, axis=1)
    s_test = np.argmax(s_test, axis=1)
    
    datasets = (h_train, h_val, h_test, y_train, y_val, y_test, s_train, s_val, s_test)


    if 'random' in train_path:
        return get_majority_predictions(datasets)

    results = {}
    results["svc"] = test_performances(klass=svm.SVC, kargs={"random_state": RANDOM_SEED, "gamma":"auto"}, datasets=datasets,)
    results["tree"] = test_performances(klass=tree.DecisionTreeClassifier, kargs={"random_state":RANDOM_SEED, "max_depth":4}, datasets=datasets )
    results["lr"] = test_performances(klass=lm.LogisticRegression, kargs={"random_state": RANDOM_SEED, "solver":"lbfgs"}, datasets=datasets)
    results["forest"] = test_performances(klass=ens.RandomForestClassifier, kargs={"random_state": RANDOM_SEED}, datasets=datasets)
    # results["mine"] = mine_performances

    return results

def estimate_mi_mine(h_train, s_train, h_val, s_val, h_test, s_test):
    mine = MINE(np.vstack([h_train, h_val, h_test]), np.vstack([s_train, s_val, s_test]))
    mine.train()
    mi_train = mine.estimate(np.vstack([h_train, h_val, h_test]), np.vstack([s_train, s_val, s_test]))
    mi_val   = mine.estimate(h_val, s_val)
    mi_test  = mine.estimate(h_test, s_test)
    mine.close_session()
    return {
        "s": {"train": float(mi_train), "val": float(mi_val), "test": float(mi_test), "discr": 0.0},
        "y": {"train": 0.0, "val": 0.0, "test": 0.0}
        }

def estimate_mi_bins(h_train, s_train, h_val, s_val, h_test, s_test):
    pass

def get_majority_predictions(datasets):
    h_train, h_val, h_test, y_train, y_val, y_test, s_train, s_val, s_test = datasets

    results = {}
    acc_s_train = max(np.bincount(s_train)) / sum(np.bincount(s_train))
    acc_y_train = max(np.bincount(y_train)) / sum(np.bincount(y_train))

    acc_s_val = max(np.bincount(s_val)) / sum(np.bincount(s_val))
    acc_y_val = max(np.bincount(y_val)) / sum(np.bincount(y_val))

    acc_s_test = max(np.bincount(s_test)) / sum(np.bincount(s_test))
    acc_y_test = max(np.bincount(y_test)) / sum(np.bincount(y_test))


    result = {
        "s": {"train": acc_s_train, "val": acc_s_val, "test": acc_s_test},
        "y": {"train": acc_y_train, "val": acc_y_val, "test": acc_y_test}
        }

    # result_mine = {
    #     "s": {"train": 0.0, "val": 0.0, "test": 0.0},
    #     "y": {"train": 0.0, "val": 0.0, "test": 0.0}
    #     }

    results["svc"]    = result
    results["tree"]   = result
    results["lr"]     = result
    results["forest"] = result
    # results["mine"] = result_mine
    return results



def test_performances(klass, kargs, datasets):
    h_train, h_val, h_test, y_train, y_val, y_test, s_train, s_val, s_test = datasets
    classifier_y = klass(**kargs)

    classifier_y.fit(h_train, y_train)
    pred_y_train = classifier_y.predict(h_train)
    pred_y_val = classifier_y.predict(h_val)
    pred_y_test = classifier_y.predict(h_test)

    acc_y_train = accuracy(pred_y_train, y_train)
    acc_y_val = accuracy(pred_y_val, y_val)
    acc_y_test = accuracy(pred_y_test, y_test)

    classifier_s = klass(**kargs)

    classifier_s.fit(h_train, s_train)

    pred_s_train = classifier_s.predict(h_train)
    pred_s_val = classifier_s.predict(h_val)
    pred_s_test = classifier_s.predict(h_test)

    acc_s_train = accuracy(pred_s_train, s_train)
    acc_s_val = accuracy(pred_s_val, s_val)
    acc_s_test = accuracy(pred_s_test, s_test)

    s_dim = max(s_train) + 1
    discrimination_s = discrimination_noprob(pred_y_test, s_test, s_dim)

    return  {
        "s": {"train": acc_s_train, "val": acc_s_val, "test": acc_s_test, "discr": discrimination_s},
        "y": {"train": acc_y_train, "val": acc_y_val, "test": acc_y_test}
        }

def experiment_stem(file):
    return re.sub(r"_train\.csv|_val\.csv|_test\.csv", "", file)

def list_experiments(dir):
    files = os.listdir(dir)
    experiment_stems = map( experiment_stem, files)
    return set(experiment_stems)

def process_dir(path):
    config_path = os.path.join(path, "config.json")
    if not os.path.exists(config_path):
        print(colored("Cannot find config file in %s -- Skipping to next directory" % path, "red"))
        return { "experiment_name": path, "error": "Cannot find config file" }


    opts = Options([None, config_path])
    representations_dir = os.path.join(path, "representations")

    if not os.path.exists(representations_dir):
        print("Directory %s does not exists." % representations_dir )
        print(colored("Skipping this directory", "red"))
        return { "experiment_name": path, "error": "Cannot find representation directory" }


    experiments_results = {}
    for experiment in list_experiments(representations_dir):
        train_path = os.path.join(representations_dir, experiment+"_train.csv")
        val_path = os.path.join(representations_dir, experiment+"_val.csv")
        test_path = os.path.join(representations_dir, experiment+"_test.csv")
        results = eval_accuracies_on_representation(train_path, val_path, test_path)
        experiments_results[experiment] = results

    if len(experiments_results) == 0:
        return { "experiment_name": path, "error": "No representation found in dir %s" % representations_dir }

    return {
        "experiment_name": os.path.abspath(path),
        "commit_id": read_commit_id(os.path.join(path, "commit-id")),
        "config": opts.config_struct(),
        "performances": experiments_results
    }


# Read files in a given representations directory and write a report about each representation
# found there.
#
# output is a json file in the format:
#
# {
#  "experiment_name": ...
#  "config": {
#     ...
#  }
#  models_performances: [
#     { model_name: ..., acc_s: ..., acc_y: ... }
#     ...
#  ]
# }

# file = "experiments/adult/adult-fair-networks-representations.csv"
#
#file = sys.argv[1]

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

results = None
dir = sys.argv[1]

try:
    if re.search(r'^_.*', dir):
        print(colored("Name of directory %s starts with _, skipping to the next" % dir, "green"))
    else:
        experiment_base_dir = dir

        print(colored("Processing directory: %s" % experiment_base_dir, "green"))
        results = process_dir(experiment_base_dir)
except:
    error_info = sys.exc_info()
    results = {
        "experiment_name": os.path.abspath(dir),
        "error": str(error_info[1]),
        "trace:": traceback.format_exc()
    }

output_path = os.path.join(dir, "performances.json")
with open(output_path, "w") as f:
    f.write(json.dumps(results, indent=4))
